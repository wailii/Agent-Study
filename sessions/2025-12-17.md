# 学习笔记 - 2025-12-17

## 🎯 Session Context (场景设定)

- **Topic:** B.1 ReAct 模式 & 需求文档助手架构
- **Duration:** ~60 mins
- **Context:** 在 Agent-Study 框架下,围绕 ReAct 在 RAG/需求文档助手中的应用做系统梳理,并落到一条可执行的数据流/状态流。

## ⚔️ The Socratic Dialogue (关键交锋)

- **Q1: ReAct 和“一次性规划好所有步骤的 Planner”比,最大的工程缺点是什么?**  
  - **My Initial Understanding:** 只是“多轮调用更贵更慢”。  
  - **The Correction/Guidance:** 不仅是成本问题,更关键是: ReAct 是高开销模式,应该作为复杂/不确定任务的专用通道,前面需要有一层意图识别/路由,否则会被简单问题拖垮整体性能。  
  - **Concept Clarity:** ReAct 更适合“边探路边决策”的场景,Planner 适合结构已知的流水线任务;工程上要通过 Router 控制哪些问题才允许进入 ReAct 循环。

- **Q2: history 在 ReAct 里到底是“日志”还是“状态”?**  
  - **My Initial Understanding:** 更偏日志,主要方便事后追踪。  
  - **The Correction/Guidance:** 在 ReAct 中 history 是核心状态,每一步 Thought/Action/Observation 都会被下游节点读取,决定下一步用什么工具、是否还需要继续查,甚至影响最终答案;如果只看最后一次 Observation,会导致重复踩坑和多步任务“失忆”。  
  - **Concept Clarity:** history 是短期记忆状态而不是旁观者日志,所有决策都应该基于完整的状态序列,而不是只看最新一帧。

- **Q3: ReAct 里的 Action 为什么必须用框架来做 JSON 校验/兜底?**  
  - **My Initial Understanding:** 让 LLM 自己在下一轮 Thought 里纠错就行。  
  - **The Correction/Guidance:** 不能把结构可靠性托付给 LLM 自觉;Action 作为“调用外部世界的指令”,必须由 Agent 框架在中间层做 JSON 校验、重试和降级,否则会让工具层直接崩掉,整体系统非常脆弱。  
  - **Concept Clarity:** LLM 负责“说想干嘛”,框架负责“把话变成可靠的结构化调用”;结构化输出是编排层责任,不是语言模型的善意承诺。

## 💡 My Takeaway (核心结论)

- **一句话本质:** ReAct = Reason + Act,本质是给模型一个带状态的 while 循环,让它在每一步先根据历史状态思考(Thought),再选择工具(Action),再用 Observation 更新状态。
- **架构分工:** Router 负责“要不要进 ReAct”;ReAct 负责“边查边想的决策循环”;Planner 负责“先拆好步骤再执行”;Critic/Reflection 负责“在交付前验货、删掉无证据结论”。
- **状态视角:** 规范的 Agent 系统应该围绕统一的 `state` 读写,Router / ReAct / Critic 都是 State Graph 上的节点,共享同一份状态(问题、历史、证据、停止原因等)。
- **RAG/需求助手落地:** 对需求文档助手而言,可以设计“意图识别 → ReAct + 需求工具层 (PRD/Jira/历史版本 diff) → 验证器”的数据流,在回答“这版需求对计费系统影响是什么”时,既能多轮查证,又能在最后一层过滤掉没有文档支撑的拍脑袋结论。

## 🕳 Knowledge Gaps (遗留盲区)

- [ ] [Medium] 还没有在真实框架 (如 LangGraph) 中实现 State Graph/节点,对节点函数签名、state 结构和中间件(hooks)的实际写法不够具体。
- [ ] [Medium] 工具层与需求平台/Jira 集成时,权限控制、超时、重试和幂等等工程细节还停留在概念级设计,需要结合具体 API 文档再推演。

## 💻 Code / Architecture Snippet

```python
# 极简 ReAct 循环伪代码 (围绕统一 state 读写)
state = {"question": q, "history": [], "step": 0}
MAX_STEPS = 5

while state["step"] < MAX_STEPS:
    thought, action = policy_llm(state)  # 决策: 基于当前状态想下一步做什么
    obs = run_tool_with_cache(action)    # 行动: 调用工具,带缓存避免重复外调
    state["history"].append({
        "thought": thought,
        "action": action,
        "observation": obs,
    })
    state["step"] += 1
    if is_enough_to_answer(state):       # 基于累计证据判断是否“信息已足够”
        break

draft_answer = answer_llm(state)         # 根据完整 history 生成草稿回答
final_answer = critic_llm(               # 验证器检查覆盖度+幻觉,产出最终回答
    question=q,
    draft=draft_answer,
    evidence=state["history"],
)
```

## 🔗 Reference

- RULES.md - 学习角色、Session 模板与追踪协议
- progress/study-tracker.md - B.1 ReAct 进度 & 知识盲区
- 本次对话中的 ReAct / 需求文档助手 Mermaid 流程图
